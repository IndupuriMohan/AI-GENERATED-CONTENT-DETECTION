# AI vs Human: Academic Essay Authenticity Challenge

This project aims to detect whether academic essays are written by humans or generated by AI using machine learning and deep learning techniques. The system is multilingual, supporting both English and Arabic inputs, with Arabic translated before classification.

## ðŸš€ Project Objective

To build a robust, multilingual system that classifies academic texts as:
- ðŸ§  Human-written
- ðŸ¤– AI-generated

## ðŸ§  Motivation

With the rise of large language models like ChatGPT, academic integrity is increasingly challenged by AI-generated essays. Traditional plagiarism checkers fall short in detecting such content. This project addresses this gap with ML-based detection techniques.

---

## ðŸ“Š Features

- English and Arabic language support
- Real-time essay classification
- High accuracy (95â€“96%) using BERT, CNN, and Random Forest
- Clean and simple frontend interface

---

## ðŸ› ï¸ Tech Stack

### ðŸ” Backend

- Python
- Pandas, NumPy, Scikit-learn
- NLTK for text preprocessing
- Models: Random Forest, CNN, BERT

### ðŸŒ Frontend

- Web form input for essay text
- Model selection
- Bilingual (English and Arabic)

### ðŸ–¥ï¸ Tools

- VS Code
- Google Colab / Jupyter Notebooks

---

## ðŸ§ª ML Models

| Model           | Accuracy | Highlights                                      |
|----------------|----------|-------------------------------------------------|
| Random Forest   | 96%      | Robust ensemble, balanced precision/recall     |
| BERT            | 95%      | Superior at capturing semantic context         |
| CNN             | Used     | Extracts local writing style and patterns       |

---

## ðŸ“‚ Dataset

- **Total samples:** 2,762
- **Features:**
  - `id`, `prompt_id`, `text`, `generated (0: Human, 1: AI)`

---

## ðŸ”„ Workflow

```mermaid
graph TD;
    A[Load Dataset] --> B[Preprocessing]
    B --> C[Model Training: RF / CNN / BERT]
    C --> D[Prediction on New Input]
    D --> E[Result: Human or AI]
